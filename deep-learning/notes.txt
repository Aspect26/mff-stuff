# 1
Information theory
- independent events should have additive Information
- less likely events have higher Information
- should be zero for events with probability 1

- shannon entropy ~ the average number of bits needed to transmit messages with certain disr.

- I(x) = log(1/P(x))
- H(P) = E_x~P(I(x)) = -SUM_x( P(x) * log(P(x) )
- Cross entropy & gibbs inequality: 
    - avg. amount of bits needed to transmit messages Q using codes for P
    - H(P, Q) = -E_x~P[logQ(x)]
    - H(P, Q) > H(P)
    - H(P) = H(P, Q) -> P = Q
    - KL divergence: Dkl ~ H(P, Q) - H(P) : metric of two distributions (slide:15/50)

Normal distribution:
- CTL: sum of indep., samely distrib. random vars with limited var converges to norm. distr.
- maximum entropy principle: for given mean and varience normal distrib. has highest entropy

Machine learning: 
- supervised: labeled data
- unsupervised: data without annotations
- reinforcement: only good/bad signals

History:
- 1940: electronic brain
- 1950s: perceptron network with training alg
- 1970: Xor problem & AI winter
    - to fix "xor problem" you need hidden network connected to _all_ inputs
    -> not enough computational power at that time to have fully connected layers
- 1980s: multi-layered perceptron, backpropagation 
- 1990s: SVM (simpler, easier to use)
- 2006: Deep nerual network 

Activation fushion:

ML:
- linear regression ~ single layer NN with MSE
- logistic regression: linear regression with sigmoid = 1/1+e^-x
- softmax for mutiple classes

- maximum entropy model: softmax: e^z1 / SUM_j e^zj
    -> model that produces the most general distribution over input data

NN:
- graph (DAG) based computational model
- universal approximation theorem '89
    - input, hidden, output layers
    - non-linear, non-cons, bounded, monoton, cont activation activation function
    - given enough neurons in one hidden layer any function can be arbitrarily approximated

- activation functions: tanh, ReLU = max(0, x), ...
- distributed representation: the alg learns features & info about them, not indiv. examples

#2:

ML:
- learn on training set to minimize errors on not-seen data (test set)

- errors
    - underfitting: too little specialized
    - overfitting: model too specialized on training set, can't generalize

- model capacity -> with higher capacity
    - training error decreases
    - test error / generalization error
        - decreases at first (needs to have enough capacity to learn) 
        - increases later (overfits and remembers the training set instead of generalizing)

- regularization: reducing generalization error
    - setting correct model capacity
    - occams razor: the simplest model generalizes the best
    
    - early stopping
    - L1, L2: we don't want high (L2) / non-zero parameters (L1:constant derivative)

- model
    - parameters: training alg. modifies them
    - hyperparameters: training alg. doesn't update them

    - we should have separate set for hyperparameters evaluation : validation set

- loss function: how well the model fits data
    - MeanSquaredError: only one local minimum, also global minimum
    - MLE: Maximum likelyhood princ.: way of desiging loss functions
        - parameters so that the probability of our data given the params is highest
        - O = argmax p_model(X, 0) = argmin E_x~pdata[-log(pmodel(x, 0))] = argmin Dkl(pdata || pmodel(x, 0)) + (H(pdata))
        -> cross-entropy or Kullback-Leibler divergence

    - If we expect normal distrib. or target var -> MLE for regression leads to MSE

Gradient descent:
- move parameters in the direction of gradient (vector of partial derivatives)
    - O <- O - a * grad_O(J)(O)

- gradient descent: compute gradient over all training data
- online gradient descent: estimate gradient with one randomly sampled example 
- minibatch SGD: tradeoff, compute gradient given m random samples 

Backprop: 
- computing gradient given the nerual network 
- what would be the effect of parameters modifications

- chain-rule & dynamic programming: dJ / dyj = dJ/ dz * dz / dyi 
- derivation w.r.t to o_i is derivative w.r.t to it * derivative of following nodes

- alg:
    - run forward prop and compute all u_i (activation of node i)
    - g_n = 1
    - for i = n-1, ..., 1:
        - g_i = SUM_j:i€P(u_j)( g_j * (du_j / du_i) )
    - return g

SGD variants: http://ruder.io/optimizing-gradient-descent/ 
- SGD with momentum
    - remember the velocity of descent
    - moves in the the direction of actual progress within the loss function space
    - exponential moving average of the velocity: v <- beta * v - alpha * g
        - gradients further in history have exp. lower contribution


    
- SGD with nesterov momentum
    - similar to momentum, update parameters before computing the gradient
    - the gradient is computed in already better position (one that used the momentum info)
    - converges faster
        
- ADA Grad
    - in SGD absolute highest wins even if some smaller has "higher signal" given its usual variance 
    - ADA-grad tries to do adaptive dimension-specific learning rate scaling
        
    - estimates second moment (Variance) of each dimension (r <- r + g^2)
    - updates with gradient scaled by relative signall strength O <- O - a * (g / sqrt(r+e)) 
        - sqrt(r+e) ~ g: their ratio shows how strong the current gradient signall is for each dimension given their usual variance

    - the r element grows ever larger -> effectively decreases learning rate w.r.t sqrt(t)
        - problematic for N.N. that train for a large number of steps

- RMS Prop:
    - similar to ADA Grad, doesn't accumulate r but computes its exponential moving average
    - scaling factor doesn't increase for ever -> works well even for NN that trains for hours

- ADAM: Adaptive momentum
    - computes exponential moving average of both first (velocity) and second (variance) moments
    - essentially RMSProp + Momentum 
    - update with momentum normalized by second momentum (variance)

    - correction for first x steps of the alg where it doesn't have historical data
        - makes both momentum estimates larger for first few steps because their historical data are 0
        - presumes the previous (non-existent) data would have similar distrib. to the currently observed
        - r <- r/(1-b^t) 

        - essentially scales learning factor: at <- alpha * sqrt(1-bs^t)/(1-br^t)
        - generally it makes effective LR faster in the beginning because we don't have reliable estimates for s and r


# 3
Parameters: 
- layer: w(Wh + b)
- parameters: Wi, bi
- Derivations w.r.t. all Wi, bi

Hyperparameters:
- e.g. topology of the network
- learning rate, dropout, ...
- can't train it via backprop due to them being discrete

- can use grid search, bayes opt, reinforcement approaches, ...

Practical problems
- processing data in batches -> overhead for moving data to GPU is significant
    - the first dimension is usually batch 

- computing the network via vector and matrixes operations
    - application of weights is matrix multiplication
    - bias is vector addition 
    - activation function is per element application of a function on vector

    - derivation is basically adding a reversed graph with derivated original nodes
    -> graph is DAG AST of vector operations 

High level overview
Architecture   | Classical '90s | Deep learning
Architecture   | :::            | ::::::::::::::::
Activation f.  | tanh, sigmoid  | tanh, ReLU, PReLU, ELU, SELU, Swish
Output f.      | none, sigmoid  | none, sigmoid, softmax
Loss f.        | MSE            | NLL (or cross-entropy or KL-convergence)
Optimization   | SGD, momentum  | SGD, RMSProp, Adam, ...
Regularization | L2, L1         | L2, Dropout, BatchNorm, LayerNorm, ...

Derivative of soft-max 
- oi = e^zi / SUM_j e^zj
- derivation of softmax is 1-<predictedProbability> for the gold class and <predProp> for others
- as soon as the model starts doing what it should do -> the derivatives start being quite small

- label smoothing: instead of having 0, 0, 0, 1, 0, 0 gold data distrib -> 0.05, 0.05, ... 0.8, 0.05, ...
    - Doesn't force the model to be perfectly sure, can combat overfitting 

- to combat overflow -> substract minimum value, softmax is invariant to additions / substractions

Regularization: 
- occam's razor -> prefer simpler models to complex ones -> less overfitting 
- the smaller the training dataset is the more prone the training is to overfitting

- idea: model capacity ~  training set size 
    - the more data I have the harder it is for the model to overfit 
    - the smaller the model's capacity is the harder it is for the model to overfit 

- idea: in the beginning the model learns general rules, moves to more specific one
    - early stopping when the validation error starts increasing for some period of time

- L2 regularaizaton: models with smaller parameters tend to be more generických
    - minimize Loss(O, X) + lambda*||O||
    - it's hard to set lambda: strength of the regularization 

    - Derivation of L2 is -2 * lambda * Oi 
    - The higher the parameter is the larger (linearly) is it's derivation -> always substract 
    - Always shrinks the parameter by some fixed percentage 

    - parameter needs to stay relevant to be reasonably high 
    - if it's used only for one batch -> gets killed by regularization 

    - L2 is based on MAP bayes with prior normal distribution on parameters that has mean 0

- L1 regularizaion: tries to create sparse parameters
    - always substracts / nulls constant from a parameter
    - doesn't work well for N.N: N.N can store a lot of information in small numbers 
        - for N.N. it's not about the absolute value of weights but their relative sizes
        - substracting constant kills that

- dataset augmentation 
    - create new train data via small semantic modifications (translations, ...) from the original train dataset 
    - possible to add noise instead of using semantic modifications
    -> forces the model to learn general features 

    - label smoothing could be viewed as injecting noise to gold data distribution 
    - better if similar classes have higher probability in the label smoothing 
    
Ensembling: 
- create combinaton of several (worse) models 
- if the models aren't perfectly correlated -> gain some information 

- due to N.N. complexity the probability that repeated learning will produce the same model is extremely small
-> train N.N. multiple times (takes too much time) / take multiple snapshots throughout learning 

- use ensembled models to annotate not-yet-annotated data -> learn new models on them
-> better training dataset for "free"

- bootstrapping: sample original dataset with replacement 
    - each model gets different sampled training set 
    - can sample individual data indivs. / parts of these data individuals 

    -> not necessary for N.N because due to complexity & random init it's easy to train different models with the same data & hyperparameters

- boosting: changing weights of models and data
    - data we can't classify well have higher weights
    - models that are better have higher weights

Dropout: 
- with certain probility we drop a neuron in a layer: multiply its output with 0
- prevents overfitting -> every neuron / feature can be dropped -> hard to remember data exactly 

- can be viewed as ensembling that trains multiple models with different topologies at the same time
    - training all models that can be created via deleting some non-output neuron 

- random bit mask for neurons for each minibatch (static probability of dropout)
- each batch is updated using the bitmask, next minibatch has a new (different mask)

- training essentially minimizes loss across all these masks (exp. many of them)
- approxed via sampling 

- creates highly correlated models (they share most of the weights) but there're exp. many of them
- due to the models share (most) weights the other parts have to be robust to work -> "independent modules"  
- ideal if every minibatch is sampled with replacement 

- alternatively it could be viewed as activation scaling with the factor of 1/(1-p)
- dropout could theoretically do other things than just 0 multiplication 

- during inference: 
    - sample different masks: ineffective 
    - approx the result via one pass trough the full network with activations scaled with 1-p 
        - scaling needs to be there so that the mean and varience of inputs at each layer is the same as in training

    - for linear regression it's the same as L2 with per input decay rate 

# 4: 
- the graph can have conditional nodes, while nodes, ...
    - tf.cond: only one branch is evaluated -> slow, hard on GPUs, ...
    - tf.where: all branches are evaluated -> faster, ...
- backpropagation understands such nodes, ...

Convergence
- not used in mathematical sense, but as "trains well" 
    - converges: learns well
    - diverges: stays at random guess level of accuracy

- saturating non-linearities
    - tanh on |x| > 2 has almost zero derivative 
    - backpropagation trough more layers (multiplication with tanh's derivative) makes gradient disappear 

    - sigmoid has value of 1/2 for 0 -> adds up to 1 very quickly -> saturates -> gradient vanishes 

- gradient exploision 
    - huge gradient can shoot us to a completely different parameters position
    - it can be benefitial to have some maximum gradient -> gradient clipping 

    - can be done per parameter -> changes gradient direction if only some are clipped
    - better to normalize it as a whole -> essentially lowering learning rate so that the update has some max. norm 

- parameter initialization 
    - want activations to have some reasoanble distribution (mean, varience)
    - can't initialize with 0 -> all neurons would be identical -> all derivatives would be the same 
    - generating random weights enables different neurons to learn different things

    - can't initialize from (-1, 1), each neuron has n predecessors -> could explode -> (-1/sqrt(n), 1/sqrt(n))
    - actually -sqrt(6/(m+n)), sqrt(6/(m+n)) works better sqrt(3) * sqrt(1 / (m+n)/2)
    - average between inputs and outputs (m+n)/2 -> to be nice for backrop as well
    - better activation distribution, variance stays the same for futher layers 

    - biases usually initialized to 0

Going deeper: 
- dense network doesn't utiliize dimensionality structure within the data
    - can't easily learn position independent feature extraction 
    - activation for certain neuron should be influenced only by its limited neighbourhood 

- cross-correlation ("convolution") operation across the data's dimensionality
    - convolution: (x <> w)t = SUM(xi * wt-i)
    - cross-corellation: moving in the other direction: (x <> w)t = SUM(xt+i * wi) //it's more visible on 2D

- kernel specifies values used in cross-corelation (wi)

- for pictures 2D special, 3rd dimension: not structural, independent information
    - convolution has kernel for each input channel (all with the same size)
    - to create multiple output channels -> multiple kernels 

    - kernel is actually 4D tensor for 2D images (2 spacial, 1 input channels, 1 output channels)

- stride: we might want to apply kernel every n pixels, not to each 
- padding: can't put kernel on the edges of the picture (would overlap) outside of data -> padding schemes
    - valid: use only valid pixels -> makes picture smaller
    - same: pad data (picture) with 0s to maintain size

- average pooling / max pooling
    - convolution with fixed, non learnable kernel 
    - average, max (doesn't actually have conv. repre)

    - almost alaways done with non-1 stride 

Architecture ~ Alexnet:
- start with small convolutions, do max pooling, repeat
- slowly starts producing higher, and higher level (more abstract) features
    - starts with lines, gradients, ..., then starts recognizing basic shapes, ...

- when the picture is small enough -> e.g. 7x7 add dense classification layer 
    - it's useful to know if the sun is in the upper / bottom half 
    - on 7x7 level there are high level features s.a. sunness, dogness, ...

- Alexnet still used bigger convolutions and not only 3x3
- whenever we do stride, create more output channels (stride 2 -> 2x channels)

- instead of dense layers in the end nowadays global average pooling is used 
    - there were too many parameters in the last dense layer that did next to nothing

- conv. networks are subset of networks that prefer local local features 
Problem: 
- convolutions on differently sized inputs produce differently sized outputs, but work
- convolution networks with average global pooling can work with any-sized input

# 5: 
2D convolution: (I * K)i,j,k = SUM_m,n,o [I_i*S+m,j*S+n,o * K_m,n,o,k]
- S stride, k output channel, o input channel

VGG 2014  
- conv3, conv3, maxpool with stride 2 and double the number of output channels in following convs
    - doubling because the complexity of conv is H * W * Ic * Oc -> stays the same
- in the end do maxpool, a few dense layers (thousands of neurons), softmax 
    - most of the parameters are actually in the dense part of the network

GoogleNet:
- each block has parallel 1x1 conv, 3x3 conv, 5x5 conv, 3x3 max pooling & one filter concatenation 
- use of 1x1 convolutions that reduce the number of channels (creates their linear combination): "bottleneck"
    - helps with performance substentially 
    - always decrease the number of input channels before every convolution 

- instead of using fully connected layers in the end -> global average pooling 
    - looses the information about object positions 
    - a one dense output layer in the end for classification -> each channel x output_layer -> way smaller

- to improve gradient flow additional classifiers along the way, learn trough them  
    - forces the network to poduce useful features even in lower layers "regularization"

BatchNormalization 
- internal covarience shift: 
    - further layers are trained (expect) certain (input) activations distributions
    - if lower layers change (due to training) their activations can change rapidally (esp. in the beginning)
    - that can undo all learning of upper layers 

    - we might want to have more stable layers activations 

- batch normalization: 
    - compute varience and mean of activations across a batch
    - normalize current activations using the approxed variance and mean -> produces N(0, 1) distribution
        - x'i = (xi - E_batch[xi]) / sqrt(var_batch[xi])

    - let the network to learn it's own explicit mean (bi) and variance (sigmi)
    - the final output just before applying non-linearity is yi = sigmi*x'i + bi

    - no need to have bias, bi creates an explicit bias itself 
    - due to only approx. the mean and varience of layer -> adds noise -> sort of dropout / regularization 

- during training the network remembers exponential average of bias and varience -> uses them for inference
    - inference is more stable, doesn't require batches 
    - inference has these parameters fixed (as are all parameters)

- enables us to use larger learning rate (x10)
    - activation functions always have reasobale (learned) distributions -> gradients can't explode as easily

- makes initialization non-issue -> layer's output are invariant to linear scaling 
- activations can be kept around zero with B.N -> prevents saturating non-linearities 

- makes values comparable in absolute sense due to stable activations distributions 
- B.W. on conv. computes mean and varience for specific channel using all pixels not per pixel 

Inception v2 and v3:
- one 5x5 conv can be done with two 3x3, but faster 
- similar blocks to GoogleNet 

ResNet: 2015
- adding more layers can actually make it worse, even with B.N. when gradients pass fine 
- it's hard for conv. N.N. to learn identity -> to just copy data 
    - if it was easy, deeper networks would work at least as good as shallow

- add residual connections: output = F(x) + x
    - if the networks wants just copy data -> makes weights for F(x) small, x is dominant
    - doesn't have to skip just one conv., can skip the whole block 

- B.N. after the residual connection joins i.e. per whole block

- to speed up 3x3 convolutions: decreases number of channels before each to 1/4 using 1x1 conv.
- after the 3x3 conv. increases the number of channels using 1x1 again to original number

Loss function landscape
- flat minimas are better for generalization
- loss isn't sensitive to small changes of activations (both weights and inputs)

- SGD that has batchsize one actually generizes fairly well 
    - updates have quite large varience -> moves erratically through the fitness landscape 
    - can converge only in flat minimas 

WideNet: 
- doesn't use bottleneck to reduce number of channels -> smaller number of layers but better
- actually works with dropout possibly due to a high number of parameters in fat convolutions 

ResNext 
- opposite of WideNet
- many parallel exteremely narrow (on reduced number of channels, e.g. 4) convolutions in each block 

NasNet
- create blocks using RNNs (block is sequence of nodes & operations) trained trough REINFORCE 
- RNN creates block description, network is constructed, trained, it's accuracy is used to reinforce the RNN 

- produced similar blocks to previous *Nets (including residual connections), better results

Transfer learning: 
- exp: lower layers capture general features, only (few) top layers are task specific 
- remove last layer, retrain on different task -> way faster learning
    - learn only the new classifier a bit, then fine tune the whole network 
    - otherwise gradients from not-learned classifier can mess up the whole network

Beyond image class: 
- text processing (1D), speech processing (1D)
- 3d object recognition (3D), temporal-spacial e.g. video for lip reading (3D)


# 6: nothing : easter monday

# 7: 

Object detection & segmentation:

- R-CNN
    - external program suggests regions where objects could be (thousands of them)
    - each segment is classified via CNN: is there an object in the region / what object is it

- fast R-CNN
    - computes features for whole image using pre-trained ImageNet 

    - each region of interrest get represented by fixed sized window 
    - maxpool r.o.i repre via a window on the features creates by the CNN for whole picture (rol pooling)

    - classify the fixed sized r.o.i representation from the features (can be quite shallow network) 
    - bounding boxes can be trained for each r.o.i per class as well 

    - bounding box smooth L1 of difference between predicted & gold, parametrized using center & w, h
        - L2: gradient is proportional to the error -> can cause problems
        - L2 near 0, then linear -> L2 with gradient clipping 

        - xr, yr, wr, hr: parameters for RoI, xr, yr, wr, hr for bounding box
        - tx = (x-xr) / wr  //same for center.y 
        - tw = log(w/wr)    // same for height 
        - L(class, t, classg, tg) = Lcls(c, c) + lambda * SUM_i€x,y,w,h smooth_L1(ti, tgi)  

    - non-maximum supression: if there's overlap between two regions with objects -> choose the one with higher p

    - evaluation using intersection over union for bounding boxes
    - evaluation: precision <> recall 
        - rank results for each class by their prob -> slowly change tresshold between it's there x isn't there
        - average precision: area under prec<>recall curve 

- faster R-CNN
    - generate RoIs using CNN 
    - create features using deep CNN network

    - try all possible positions of reasonable regions (different rations, different size) of the features map
        - classify each if it's good -> shallow classification 
    - use rol-allign instead of rol pooling: bilinear interpolation instead of max pooling 

Normalization: 
- batchnorm: normalization across positions & within batch 
    - requires decently sized batches 
- layer norm: normalization across positions and all channels 
    - makes all channels compete with eatch other 
    - makes results worse for independent channels 
- group norm: normalization across positions and some channels 
    - groups of channels can still be independent
    - doesn't require large batch size 

    - can't add grouping for fine-tuning to a network trained without it

RNNs: 
- instead of locality uses sequentiality 
- the same network copied for each element in sequence -> output of element ith is part of input for i+1th

- with RNN the exploding/vanishing gradient is worse -> problem of long dependencies 
    - due to the fact that the same network is coppied multiple times -> exponential transformation 
    - "solved" trough LSTM / GRU cells that handle gradient better 

- in RNNs the state can theoretically remember any historical data -> not just fixed history window 
- to get fixed sized representation of the whole sequence: use the last output / state 

 - sequence prediction: inputs are selected previous outputs (after argmax)
    - during training: actually i-1th elements from gold data regardless on what's predicted 
    - during inference: true predicted previous outuput

- the compute graph should contain the network unrolled 
    - dynamic: unrolled in the beginning of runtime based on known input size 
    - static: unrolled on graph creation 
    
# 8: 

# 9:

# 10: 

# 11:

# 12:

# 13: 

