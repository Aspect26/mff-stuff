# 1
Information theory
- independent events should have additive Information
- less likely events have higher Information
- should be zero for events with probability 1

- shannon entropy ~ the average number of bits needed to transmit messages with certain disr.

- I(x) = log(1/P(x))
- H(P) = E_x~P(I(x)) = -SUM_x( P(x) * log(P(x) )
- Cross entropy & gibbs inequality: 
    - avg. amount of bits needed to transmit messages Q using codes for P
    - H(P, Q) = -E_x~P[logQ(x)]
    - H(P, Q) > H(P)
    - H(P) = H(P, Q) -> P = Q
    - KL divergence: Dkl ~ H(P, Q) - H(P) : metric of two distributions (slide:15/50)

Normal distribution:
- CTL: sum of indep., samely distrib. random vars with limited var converges to norm. distr.
- maximum entropy principle: for given mean and varience normal distrib. has highest entropy

Machine learning: 
- supervised: labeled data
- unsupervised: data without annotations
- reinforcement: only good/bad signals

History:
- 1940: electronic brain
- 1950s: perceptron network with training alg
- 1970: Xor problem & AI winter
    - to fix "xor problem" you need hidden network connected to _all_ inputs
    -> not enough computational power at that time to have fully connected layers
- 1980s: multi-layered perceptron, backpropagation 
- 1990s: SVM (simpler, easier to use)
- 2006: Deep nerual network 

Activation fushion:

ML:
- linear regression ~ single layer NN with MSE
- logistic regression: linear regression with sigmoid = 1/1+e^-x
- softmax for mutiple classes

- maximum entropy model: softmax: e^z1 / SUM_j e^zj
    -> model that produces the most general distribution over input data

NN:
- graph (DAG) based computational model
- universal approximation theorem '89
    - input, hidden, output layers
    - non-linear, non-cons, bounded, monoton, cont activation activation function
    - given enough neurons in one hidden layer any function can be arbitrarily approximated

- activation functions: tanh, ReLU = max(0, x), ...
- distributed representation: the alg learns features & info about them, not indiv. examples

#2:

ML:
- learn on training set to minimize errors on not-seen data (test set)

- errors
    - underfitting: too little specialized
    - overfitting: model too specialized on training set, can't generalize

- model capacity -> with higher capacity
    - training error decreases
    - test error / generalization error
        - decreases at first (needs to have enough capacity to learn) 
        - increases later (overfits and remembers the training set instead of generalizing)

- regularization: reducing generalization error
    - setting correct model capacity
    - occams razor: the simplest model generalizes the best
    
    - early stopping
    - L1, L2: we don't want high (L2) / non-zero parameters (L1:constant derivative)

- model
    - parameters: training alg. modifies them
    - hyperparameters: training alg. doesn't update them

    - we should have separate set for hyperparameters evaluation : validation set

- loss function: how well the model fits data
    - MeanSquaredError: only one local minimum, also global minimum
    - MLE: Maximum likelyhood princ.: way of desiging loss functions
        - parameters so that the probability of our data given the params is highest
        - O = argmax p_model(X, 0) = argmin E_x~pdata[-log(pmodel(x, 0))] = argmin Dkl(pdata || pmodel(x, 0)) + (H(pdata))
        -> cross-entropy or Kullback-Leibler divergence

    - If we expect normal distrib. or target var -> MLE for regression leads to MSE

Gradient descent:
- move parameters in the direction of gradient (vector of partial derivatives)
    - O <- O - a * grad_O(J)(O)

- gradient descent: compute gradient over all training data
- online gradient descent: estimate gradient with one randomly sampled example 
- minibatch SGD: tradeoff, compute gradient given m random samples 

Backprop: 
- computing gradient given the nerual network 
- what would be the effect of parameters modifications

- chain-rule & dynamic programming: dJ / dyj = dJ/ dz * dz / dyi 
- derivation w.r.t to o_i is derivative w.r.t to it * derivative of following nodes

- alg:
    - run forward prop and compute all u_i (activation of node i)
    - g_n = 1
    - for i = n-1, ..., 1:
        - g_i = SUM_j:i€P(u_j)( g_j * (du_j / du_i) )
    - return g

SGD variants: http://ruder.io/optimizing-gradient-descent/ 
- SGD with momentum
    - remember the velocity of descent
    - moves in the the direction of actual progress within the loss function space
    - exponential moving average of the velocity: v <- beta * v - alpha * g
        - gradients further in history have exp. lower contribution


    
- SGD with nesterov momentum
    - similar to momentum, update parameters before computing the gradient
    - the gradient is computed in already better position (one that used the momentum info)
    - converges faster
        
- ADA Grad
    - in SGD absolute highest wins even if some smaller has "higher signal" given its usual variance 
    - ADA-grad tries to do adaptive dimension-specific learning rate scaling
        
    - estimates second moment (Variance) of each dimension (r <- r + g^2)
    - updates with gradient scaled by relative signall strength O <- O - a * (g / sqrt(r+e)) 
        - sqrt(r+e) ~ g: their ratio shows how strong the current gradient signall is for each dimension given their usual variance

    - the r element grows ever larger -> effectively decreases learning rate w.r.t sqrt(t)
        - problematic for N.N. that train for a large number of steps

- RMS Prop:
    - similar to ADA Grad, doesn't accumulate r but computes its exponential moving average
    - scaling factor doesn't increase for ever -> works well even for NN that trains for hours

- ADAM: Adaptive momentum
    - computes exponential moving average of both first (velocity) and second (variance) moments
    - essentially RMSProp + Momentum 
    - update with momentum normalized by second momentum (variance)

    - correction for first x steps of the alg where it doesn't have historical data
        - makes both momentum estimates larger for first few steps because their historical data are 0
        - presumes the previous (non-existent) data would have similar distrib. to the currently observed
        - r <- r/(1-b^t) 

        - essentially scales learning factor: at <- alpha * sqrt(1-bs^t)/(1-br^t)
        - generally it makes effective LR faster in the beginning because we don't have reliable estimates for s and r


# 3
Parameters: 
- layer: w(Wh + b)
- parameters: Wi, bi
- Derivations w.r.t. all Wi, bi

Hyperparameters:
- e.g. topology of the network
- learning rate, dropout, ...
- can't train it via backprop due to them being discrete

- can use grid search, bayes opt, reinforcement approaches, ...

Practical problems
- processing data in batches -> overhead for moving data to GPU is significant
    - the first dimension is usually batch 

- computing the network via vector and matrixes operations
    - application of weights is matrix multiplication
    - bias is vector addition 
    - activation function is per element application of a function on vector

    - derivation is basically adding a reversed graph with derivated original nodes
    -> graph is DAG AST of vector operations 

High level overview
Architecture   | Classical '90s | Deep learning
Architecture   | :::            | ::::::::::::::::
Activation f.  | tanh, sigmoid  | tanh, ReLU, PReLU, ELU, SELU, Swish
Output f.      | none, sigmoid  | none, sigmoid, softmax
Loss f.        | MSE            | NLL (or cross-entropy or KL-convergence)
Optimization   | SGD, momentum  | SGD, RMSProp, Adam, ...
Regularization | L2, L1         | L2, Dropout, BatchNorm, LayerNorm, ...

Derivative of soft-max 
- oi = e^zi / SUM_j e^zj
- derivation of softmax is 1-<predictedProbability> for the gold class and <predProp> for others
- as soon as the model starts doing what it should do -> the derivatives start being quite small

- label smoothing: instead of having 0, 0, 0, 1, 0, 0 gold data distrib -> 0.05, 0.05, ... 0.8, 0.05, ...
    - Doesn't force the model to be perfectly sure, can combat overfitting 

- to combat overflow -> substract minimum value, softmax is invariant to additions / substractions

Regularization: 
- occam's razor -> prefer simpler models to complex ones -> less overfitting 
- the smaller the training dataset is the more prone the training is to overfitting

- idea: model capacity ~  training set size 
    - the more data I have the harder it is for the model to overfit 
    - the smaller the model's capacity is the harder it is for the model to overfit 

- idea: in the beginning the model learns general rules, moves to more specific one
    - early stopping when the validation error starts increasing for some period of time

- L2 regularaizaton: models with smaller parameters tend to be more generických
    - minimize Loss(O, X) + lambda*||O||
    - it's hard to set lambda: strength of the regularization 

    - Derivation of L2 is -2 * lambda * Oi 
    - The higher the parameter is the larger (linearly) is it's derivation -> always substract 
    - Always shrinks the parameter by some fixed percentage 

    - parameter needs to stay relevant to be reasonably high 
    - if it's used only for one batch -> gets killed by regularization 

    - L2 is based on MAP bayes with prior normal distribution on parameters that has mean 0

- L1 regularizaion: tries to create sparse parameters
    - always substracts / nulls constant from a parameter
    - doesn't work well for N.N: N.N can store a lot of information in small numbers 
        - for N.N. it's not about the absolute value of weights but their relative sizes
        - substracting constant kills that

- dataset augmentation 
    - create new train data via small semantic modifications (translations, ...) from the original train dataset 
    - possible to add noise instead of using semantic modifications
    -> forces the model to learn general features 

    - label smoothing could be viewed as injecting noise to gold data distribution 
    - better if similar classes have higher probability in the label smoothing 
    
Ensembling: 
- create combinaton of several (worse) models 
- if the models aren't perfectly correlated -> gain some information 

- due to N.N. complexity the probability that repeated learning will produce the same model is extremely small
-> train N.N. multiple times (takes too much time) / take multiple snapshots throughout learning 

- use ensembled models to annotate not-yet-annotated data -> learn new models on them
-> better training dataset for "free"

- bootstrapping: sample original dataset with replacement 
    - each model gets different sampled training set 
    - can sample individual data indivs. / parts of these data individuals 

    -> not necessary for N.N because due to complexity & random init it's easy to train different models with the same data & hyperparameters

- boosting: changing weights of models and data
    - data we can't classify well have higher weights
    - models that are better have higher weights

Dropout: 
- with certain probility we drop a neuron in a layer: multiply its output with 0
- prevents overfitting -> every neuron / feature can be dropped -> hard to remember data exactly 

- can be viewed as ensembling that trains multiple models with different topologies at the same time
    - training all models that can be created via deleting some non-output neuron 

- random bit mask for neurons for each minibatch (static probability of dropout)
- each batch is updated using the bitmask, next minibatch has a new (different mask)

- training essentially minimizes loss across all these masks (exp. many of them)
- approxed via sampling 

- creates highly correlated models (they share most of the weights) but there're exp. many of them
- due to the models share (most) weights the other parts have to be robust to work -> "independent modules"  
- ideal if every minibatch is sampled with replacement 

- alternatively it could be viewed as activation scaling with the factor of 1/(1-p)
- dropout could theoretically do other things than just 0 multiplication 

- during inference: 
    - sample different masks: ineffective 
    - approx the result via one pass trough the full network with activations scaled with 1-p 
        - scaling needs to be there so that the mean and varience of inputs at each layer is the same as in training

    - for linear regression it's the same as L2 with per input decay rate 

# 4: 
- the graph can have conditional nodes, while nodes, ...
    - tf.cond: only one branch is evaluated -> slow, hard on GPUs, ...
    - tf.where: all branches are evaluated -> faster, ...
- backpropagation understands such nodes, ...

Convergence
- not used in mathematical sense, but as "trains well" 
    - converges: learns well
    - diverges: stays at random guess level of accuracy

- saturating non-linearities
    - tanh on |x| > 2 has almost zero derivative 
    - backpropagation trough more layers (multiplication with tanh's derivative) makes gradient disappear 

    - sigmoid has value of 1/2 for 0 -> adds up to 1 very quickly -> saturates -> gradient vanishes 

- gradient exploision 
    - huge gradient can shoot us to a completely different parameters position
    - it can be benefitial to have some maximum gradient -> gradient clipping 

    - can be done per parameter -> changes gradient direction if only some are clipped
    - better to normalize it as a whole -> essentially lowering learning rate so that the update has some max. norm 

- parameter initialization 
    - want activations to have some reasoanble distribution (mean, varience)
    - can't initialize with 0 -> all neurons would be identical -> all derivatives would be the same 
    - generating random weights enables different neurons to learn different things

    - can't initialize from (-1, 1), each neuron has n predecessors -> could explode -> (-1/sqrt(n), 1/sqrt(n))
    - actually -sqrt(6/(m+n)), sqrt(6/(m+n)) works better sqrt(3) * sqrt(1 / (m+n)/2)
    - average between inputs and outputs (m+n)/2 -> to be nice for backrop as well
    - better activation distribution, variance stays the same for futher layers 

    - biases usually initialized to 0

Going deeper: 
- dense network doesn't utiliize dimensionality structure within the data
    - can't easily learn position independent feature extraction 
    - activation for certain neuron should be influenced only by its limited neighbourhood 

- cross-correlation ("convolution") operation across the data's dimensionality
    - convolution: (x <> w)t = SUM(xi * wt-i)
    - cross-corellation: moving in the other direction: (x <> w)t = SUM(xt+i * wi) //it's more visible on 2D

- kernel specifies values used in cross-corelation (wi)

- for pictures 2D special, 3rd dimension: not structural, independent information
    - convolution has kernel for each input channel (all with the same size)
    - to create multiple output channels -> multiple kernels 

    - kernel is actually 4D tensor for 2D images (2 spacial, 1 input channels, 1 output channels)

- stride: we might want to apply kernel every n pixels, not to each 
- padding: can't put kernel on the edges of the picture (would overlap) outside of data -> padding schemes
    - valid: use only valid pixels -> makes picture smaller
    - same: pad data (picture) with 0s to maintain size

- average pooling / max pooling
    - convolution with fixed, non learnable kernel 
    - average, max (doesn't actually have conv. repre)

    - almost alaways done with non-1 stride 

Architecture ~ Alexnet:
- start with small convolutions, do max pooling, repeat
- slowly starts producing higher, and higher level (more abstract) features
    - starts with lines, gradients, ..., then starts recognizing basic shapes, ...

- when the picture is small enough -> e.g. 7x7 add dense classification layer 
    - it's useful to know if the sun is in the upper / bottom half 
    - on 7x7 level there are high level features s.a. sunness, dogness, ...

- Alexnet still used bigger convolutions and not only 3x3
- whenever we do stride, create more output channels (stride 2 -> 2x channels)

- instead of dense layers in the end nowadays global average pooling is used 
    - there were too many parameters in the last dense layer that did next to nothing

- conv. networks are subset of networks that prefer local local features 
Problem: 
- convolutions on differently sized inputs produce differently sized outputs, but work
- convolution networks with average global pooling can work with any-sized input

# 5: 
2D convolution: (I * K)i,j,k = SUM_m,n,o [I_i*S+m,j*S+n,o * K_m,n,o,k]
- S stride, k output channel, o input channel

VGG 2014  
- conv3, conv3, maxpool with stride 2 and double the number of output channels in following convs
    - doubling because the complexity of conv is H * W * Ic * Oc -> stays the same
- in the end do maxpool, a few dense layers (thousands of neurons), softmax 
    - most of the parameters are actually in the dense part of the network

GoogleNet:
- each block has parallel 1x1 conv, 3x3 conv, 5x5 conv, 3x3 max pooling & one filter concatenation 
- use of 1x1 convolutions that reduce the number of channels (creates their linear combination): "bottleneck"
    - helps with performance substentially 
    - always decrease the number of input channels before every convolution 

- instead of using fully connected layers in the end -> global average pooling 
    - looses the information about object positions 
    - a one dense output layer in the end for classification -> each channel x output_layer -> way smaller

- to improve gradient flow additional classifiers along the way, learn trough them  
    - forces the network to poduce useful features even in lower layers "regularization"

BatchNormalization 
- internal covarience shift: 
    - further layers are trained (expect) certain (input) activations distributions
    - if lower layers change (due to training) their activations can change rapidally (esp. in the beginning)
    - that can undo all learning of upper layers 

    - we might want to have more stable layers activations 

- batch normalization: 
    - compute varience and mean of activations across a batch
    - normalize current activations using the approxed variance and mean -> produces N(0, 1) distribution
        - x'i = (xi - E_batch[xi]) / sqrt(var_batch[xi])

    - let the network to learn it's own explicit mean (bi) and variance (sigmi)
    - the final output just before applying non-linearity is yi = sigmi*x'i + bi

    - no need to have bias, bi creates an explicit bias itself 
    - due to only approx. the mean and varience of layer -> adds noise -> sort of dropout / regularization 

- during training the network remembers exponential average of bias and varience -> uses them for inference
    - inference is more stable, doesn't require batches 
    - inference has these parameters fixed (as are all parameters)

- enables us to use larger learning rate (x10)
    - activation functions always have reasobale (learned) distributions -> gradients can't explode as easily

- makes initialization non-issue -> layer's output are invariant to linear scaling 
- activations can be kept around zero with B.N -> prevents saturating non-linearities 

- makes values comparable in absolute sense due to stable activations distributions 
- B.W. on conv. computes mean and varience for specific channel using all pixels not per pixel 

Inception v2 and v3:
- one 5x5 conv can be done with two 3x3, but faster 
- similar blocks to GoogleNet 

ResNet: 2015
- adding more layers can actually make it worse, even with B.N. when gradients pass fine 
- it's hard for conv. N.N. to learn identity -> to just copy data 
    - if it was easy, deeper networks would work at least as good as shallow

- add residual connections: output = F(x) + x
    - if the networks wants just copy data -> makes weights for F(x) small, x is dominant
    - doesn't have to skip just one conv., can skip the whole block 

- B.N. after the residual connection joins i.e. per whole block

- to speed up 3x3 convolutions: decreases number of channels before each to 1/4 using 1x1 conv.
- after the 3x3 conv. increases the number of channels using 1x1 again to original number

Loss function landscape
- flat minimas are better for generalization
- loss isn't sensitive to small changes of activations (both weights and inputs)

- SGD that has batchsize one actually generizes fairly well 
    - updates have quite large varience -> moves erratically through the fitness landscape 
    - can converge only in flat minimas 

WideNet: 
- doesn't use bottleneck to reduce number of channels -> smaller number of layers but better
- actually works with dropout possibly due to a high number of parameters in fat convolutions 

ResNext 
- opposite of WideNet
- many parallel exteremely narrow (on reduced number of channels, e.g. 4) convolutions in each block 

NasNet
- create blocks using RNNs (block is sequence of nodes & operations) trained trough REINFORCE 
- RNN creates block description, network is constructed, trained, it's accuracy is used to reinforce the RNN 

- produced similar blocks to previous *Nets (including residual connections), better results

Transfer learning: 
- exp: lower layers capture general features, only (few) top layers are task specific 
- remove last layer, retrain on different task -> way faster learning
    - learn only the new classifier a bit, then fine tune the whole network 
    - otherwise gradients from not-learned classifier can mess up the whole network

Beyond image class: 
- text processing (1D), speech processing (1D)
- 3d object recognition (3D), temporal-spacial e.g. video for lip reading (3D)


# 6: nothing : easter monday

# 7: 

Object detection & segmentation:

- R-CNN
    - external program suggests regions where objects could be (thousands of them)
    - each segment is classified via CNN: is there an object in the region / what object is it

- fast R-CNN
    - computes features for whole image using pre-trained ImageNet 

    - each region of interrest get represented by fixed sized window 
    - maxpool r.o.i repre via a window on the features creates by the CNN for whole picture (rol pooling)

    - classify the fixed sized r.o.i representation from the features (can be quite shallow network) 
    - bounding boxes can be trained for each r.o.i per class as well 

    - bounding box smooth L1 of difference between predicted & gold, parametrized using center & w, h
        - L2: gradient is proportional to the error -> can cause problems
        - L2 near 0, then linear -> L2 with gradient clipping 

        - xr, yr, wr, hr: parameters for RoI, xr, yr, wr, hr for bounding box
        - tx = (x-xr) / wr  //same for center.y 
        - tw = log(w/wr)    // same for height 
        - L(class, t, classg, tg) = Lcls(c, c) + lambda * SUM_i€x,y,w,h smooth_L1(ti, tgi)  

    - non-maximum supression: if there's overlap between two regions with objects -> choose the one with higher p

    - evaluation using intersection over union for bounding boxes
    - evaluation: precision <> recall 
        - rank results for each class by their prob -> slowly change tresshold between it's there x isn't there
        - average precision: area under prec<>recall curve 

- faster R-CNN
    - generate RoIs using CNN 
    - create features using deep CNN network

    - try all possible positions of reasonable regions (different rations, different size) of the features map
        - classify each if it's good -> shallow classification 
    - use rol-allign instead of rol pooling: bilinear interpolation instead of max pooling 

Normalization: 
- batchnorm: normalization across positions & within batch 
    - requires decently sized batches 
- layer norm: normalization across positions and all channels 
    - makes all channels compete with eatch other 
    - makes results worse for independent channels 
- group norm: normalization across positions and some channels 
    - groups of channels can still be independent
    - doesn't require large batch size 

    - can't add grouping for fine-tuning to a network trained without it

RNNs: 
- instead of locality uses sequentiality 
- the same network copied for each element in sequence -> output of element ith is part of input for i+1th

- with RNN the exploding/vanishing gradient is worse -> problem of long dependencies 
    - due to the fact that the same network is coppied multiple times -> exponential transformation 
    - "solved" trough LSTM / GRU cells that handle gradient better 

- in RNNs the state can theoretically remember any historical data -> not just fixed history window 
- to get fixed sized representation of the whole sequence: use the last output / state 

 - sequence prediction: inputs are selected previous outputs (after argmax)
    - during training: actually i-1th elements from gold data regardless on what's predicted 
    - during inference: true predicted previous outuput

- the compute graph should contain the network unrolled 
    - dynamic: unrolled in the beginning of runtime based on known input size 
    - static: unrolled on graph creation 

- for dynamic rnn the network gets unrolled for the longest sequence 
- most operations can take seq_lengths argument -> shorter senteces are just padded 
- for GPUs the length of a sequence is more important than how wide a batch is -> len has to be computed sequentialy
    - backet sentences of similar lenth together to a batch

# 8: 
Challenge of long-term dependencies
- if partial derivatives are < 1 -> gradient in RNNs will vanish, > 1 -> it will explode
- residual connection that just copies the state -> partial derivative of 1
    - we need mechanism to read & write from this memory
    

- LSTM cell 
    - input gate: how much of input should be added to memory
    - output gate: how much of the state should we output 
    - forget gate: what parts of the state should be forgotten 

    - network decides what dimensions of the state should be updated / kept the same given input & prev. state
    - without the forget gate the memory just accumulated information -> problem

    - it <- sigm(Wi*xt + Vi*ht-1 + bi)                        // input gate
    - ot <- sigm(Wo*xt + Vo*ht-1 + bo)                        // output gate
    - ft <- sigm(Wf*xt + Vf*ht-1 + bf)                        // forget gate
    - ct <- ft * ct-1 + it * tanh(Wy * xt + Vy*ht-1 + by)     // state
    - ht <- ot * tanh(ct)                                     // output

- GRU 
    - smaller version of LSTM 

    - forgetting and updating tied together: (input: a, forget: 1-a)
    - no explicit state cell: the state is both state and output 

    - rt <- sigm(Wr*xt + Vr*ht-1 + br)          // remember gate
    - ut <- sigm(Wu*xt + Vu*ht-1 + bu)          // update
    - h't <- tanh(Wh*xt + rt * Vh*ht-1 + bh)    // updated state
    - ht <- ut * ht-1 + (1-ut) * h't            // new state

Word embeddings: 
- represeting words is hard in nerual network 

- onehot representation: unary representation of word index in vocabulary 
    - all words are independent to each other in this encoding -> can't transfer knowledge between similar words
    - too big, problem with words that are not in dictionary 

- distributed representation: 
    - each word in a fixed siyed vector from R^d: represents a space of underlying notions / meanings 
    - more similar words have vectors with smaller distance 

    - the newtork doesn't learn rules for words but for underlying features -> distributed knowledge 

- learning distributed representation
    - start with onehot represetantation 
    - create a matrix |V| * d -> W*oneHot -> distributed representation 
    - instead of multiplying the W matrix with onehot use optimized row selection op instead 

    - doesn't help with unknown words
    - enables us to learn task-specific word embeddings 

- Word2vec - Mikolov 
    - distribution hypothesis: words used in similar context are usually similar 
    - CBOW: predict word using its context 
        - sum embeddings of a word's context -> fully connected -> one hot of the word
    - skip-gram: predict context given a word
        - embed word -> fully connected layer -> predict distribution of one-hot words that can be in context 
        - trained using random words from a word's context as the gold data 

    - classification via softmax can be extremely time consuming: softmax over the whole vocabulary
    - instead of using softmax we could have "binary search hierarchial softmax"
    - instead of softmax -> sigmoids for each words : 
        - hard to select one good word -> the ones most probable -> we don't really need that
        - teach via positive & negative sampling, select some words from context that are / aren't there
        - learn the corresponding sigmoids to be 1 (for words from context) and 0 for those that aren't there

        - more negative than positive examples 
        - sample negative examples with unigram (how often are they in the text) distribution to 3/4 
        
    - emeddings are compositional 
        -  (emb(king) - emb(man)) + emb(woman) = emb(queen)

    - simple architecture -> enables us to process loads of words 
        - some words can be very infrequent 
        - being able to process a lot of data is way more important than being powerful 

- it's useful to have both precomputed embedding and learned task specific embedding 

Basic NPL processing
- start with word embeddings 

- using only left context might not be enough -> we usually have whole sentences ready
- bidirectional RNN -> two RNNs with opposite directions 
    - concatenation or addition of the resulting states / outputs


Word mbeddings for unkwnon words 
- recurrent character level embeddings using bidirRNNs on words' characters 
- convolutional character level WE: several fixed kernels, then global max/avg pooling to get fixed repre 

- works great for unknown words that are similar to ones in training set 
- both can actually capture certain sementic features of the words 

- character n-grams: 
    - word embedding is a sum of the word embeddings plus embeddings of its character n-grams
    - ngram embeddings can be stored in hashtable / oter means -> fast 


# 9:
Highway networks:
- key ingredient to train deep networks is to have mechanism to copy data
- pre-resnet, for dense / convo networks 

- highway networks: add residual connections with gating 
- with highway connections, the network still works even if we switch some layers off 

- without residual connections the network has no reason to keep certain features in their original dimensions 
- the residual connections force the network to keep the semantic space similar (so that the addition works fine)

- multilayer RNNs need residual connections (anything deeper than 2)
    - good RNNs have "residual connection" among the time axis but not among the depth axis 

Multitask learning: 
- reusing a shared parts of networks for multiple tasks 
- shared embeddings, shared encoders, ...

- traditional stacking: higher level task's backprop. goes only trough the task's specific part
    - using strong outputs from lower levels as inputs for higher
    - errors accumulate
- stack-propagation: always backrop all the way down 
    - use hidden states of lower levels as inputs -> probability distributions
    - enables us to backrop all the way down 

    - sort of "additional classifiers" along the way
    - the additional classifiers "normalize" the network 
    -> prefers models with hidden states that can do some similar tasks as well 

Regularizing RNNs
- dropout doesn't work on hidden state path -> exponentially small prop that certain dimension survives
- dropout works well for inputs and outputs

- variational dropout: the same mask for all timesteps -> solves the expontantially small prop problem 
- recurrent dropout: dropout only candidate states (only the state update)
- zoneout: randomly preserve original state instead of update -> probabilistic residual connection 

- batchnorm: doesn't work very well -> exteremely sensitive to proper initialization 
    - only a small range of game (default variance) doesn't lead to exploding / vanishing gradient
- layer norm: much more stable 

Another look at embeddings:
- instead of having A_oh * MatrixInGRU -> factorize into smaller A_oh * Smaller * MatrixInGRU 
- GRU matrixes can be substentially smaller, only one large, the subsequential one are smaller 

Sequence to sequence architecture 

- Encoder&decoder
    - encoder: creates fixed sized representation of the sentence: bidir RNN -> state
    - decoder: creates sequnce seeded with the sentence repre: previous state & last predicted -> element & new state

    - in training the decoder is fed with actual golden characters instead of what was last predicted 
    - softmax over dictionary on the decoder's output / reverse embeddings lookup 

    - fixed sized vocabulary -> problem 
    - whole sequence has to be pushed trough the fixed sized representation regardless of its length
    - softmax / reverse embedding is exteremely costly

- attention mechanism
    - translate word to the output via looking at some word in the input and it's arbitrary context 
    - we don't have data for the attention part : we don't know what words & context to use for translation 

    - can't choose discrete words -> not differentiable, must produce distribution of words to attend to 
    - bidir encoder, concat/sum of their outputs + previous decoder state are used to produce the attention distr.
        - eij = v * tanh(Vhj, Wsi-1 + b) // si-1: decoder state, j: ecnoder's output for jth input word
        - attention distrib is softmax_j(e_ij)

    - the embedded inputs are weight-summed according to the attention distribution and fed as input for decoder
    - decoder is basically the same  

    - attention can actually be used exclusively withou RNNs 

Translation for subword units 
- BPE: 
    - greedy algs

    - start with individual letter 
    - always join two units that are most freq together
    - stop when dict. small enough 

- wordpieces
    - similar heuristic for joining neighboring symbols 
    - maximizes likelyhood under unigram language model 

- usually generate one combined subword units -> unknown words get segmented the same way
- enables us to translate worlds we haven't seen yet if they are composed of parts we have seen 

# 10: 
Deep generative models: 
- generate new imagies from training data distribution using "latent vector" describing features within the dist.

- AutoEncoder: 
    - input, bottleneck with smaller dimensionaility, output
    - tries learn do identity trough the bottleneck layer 
    - bottleneck can be used as a latent vector 

    - second part of the autoencoder can generate new imagies using random latent vectors 
    - works well only for some good latent vectors that appeared during training, not for all from R^d

    - we need to train the decoder to work with the whole latent vector space
    -  not only with a few sparse points that correspond to training data inputs 

- VariationalEncoder 
    - encoder doesn't predict just one z but a (normal) distribution of zs -> a shpere of possible zs 
    - want the distributions for all training data cover whole latent vector space 
        - the encoder learns how to work "well" for all possible latent vectors, not just few points 
    
    - training: the decoder samples the z-distribution, picks one sample -> uses it to create image 
    - generation / inference: generate image using one latent vector 

    - loss: 
        - reconstruction loss: recunstruct the image as best as possible 
        - latent loss: the distributions to cover the whole latent vec. space / be invariant on input
            - KL between the distrib. of all sampled l.v. within a batch and N(0, 1) (space I want to cover) 

        - L(O, S, x) = P0(x|z) = E_QS(z|x)[log(PO(x,z))] - log(QS(z|x)) // bayes, 
                    = logP0(x) - DKL(Qfi(z|x)||Po(z)) <= P0(x)
                    = E_QS(z|x)[log(PO(x|z))] - DKL(Qfi(z|x)||Po(z)) // reconstruction, latent 
            - P(x|z) represents the distribution, the generated picture 
        
        - both losses needs to be in balance 
            - too high latent loss weight: all distrs. are N(0, 1) -> model predicts "average of all pictures"
            - too high reconstruction: collapses to AutoEncoder situation    

        - TODO: Go back to 23 & math behind loss


    - to enable the gradient flow trough sampling z: sample N(0, 1), compute mean + var * sampledVal
        - gradients can flow to both var and mean, i.e. differentiable w.r.t mean and var 
        - effective sampled distribution is N(mean, var)

    - enables interpolating between latent vectors -> produces "feature-wise" interpolated outputs 
    - training updates both encoder and decoder using one gradient descent 

    - generated pictures tend to be blury -> hard to fix 

- Generative Adversial Network:
    - generator: takes latient vector z generates data x
    - discriminator: takes either data from golden set or from generator, guesses where it comes from 

    - play a game against each other
        - generator tries to produce output that is indistinguishable from real data
        - discriminator tries to find errors in the generated data that distinguish them from real data 

    - training: the gradient for generator can flow trough distriminator first 
        - updates correpond exacatly to changes "so that it's more likely to be flagged as real data" 
    
    - two sets of trainings:
        - generator: loss: cross-entropy for "it's golden data", not updating disriminator, just flow grad. trough it
        - discriminator: cross-entropy for correct golden/generated output from distriminator, updates only discr.

    - isn't guaranteed to converge, can diverge very quickly 
        - hard to detect
        - the network can still look at if it was learning 

    - it's good to have have a batch norm in the discriminator 
        - computes whether the distribution of data across current batch is similar to gold data
        - essentially provides information about whole distribution instead of just one sample 
        - forces the generator to not to produce e.g. for mnist just 1 but be close to original distrib.

        - prevents model collapse 

    - instead of using hand-written loss we use distriminator that produces loss based on REINFORCE signals 
        
    - both generator and discriminator can use convolutions 
        - generaror actually needs inverse of convolutions -> upscale the image from smaller representation 
        - https://github.com/vdumoulin/conv_arithmetic
        - many subpixels created using 0-padded input 

    - even GAN embeds structural information to the latent vector 
        - despite it not being done as explicetely as in VAE
        - can do embedding arithmetics womanWithGlasses-woman + man -> photo of man with glasses 


    - combat divergence: consider historical versions of distriminators & generators 
        - remember old discriminators, sometimes judge using old ones 
        - have regularization that prevents updates that would change disrcim. parameters too much from histor. 
        - label smoothing: if discriminator overfits -> saturated nonlinearities -> problem, smoothing helps 

    - minibatch distrimination
        - the discriminator gets explicit information about how each image is distant from all other images on input
        - "explicit" information about current input distribution 

        - compute distances using semi-shallow NN  

    - wasserstein GAN:
        - uses better metric for minimazing generated and original distributions difference 
        
        - instead of generating probability the discriminator produces num: higher -> more likely generated 
        - discriminator (critique) must be lipschitz function <> weight clipping 
            - crude, but works 
        
        - gradients are more linear in the whole space -> learns way better 
        - value of their loss supposedly correlates to quality of the generated images 

- Hard to evaluate: 
    - looking at data and saying what outputs are better 
    - idea (VAE): better models could have better representation for the latent space 
        - instead of generating output, generate latent vector, train classifier on it 
        - the better the generative model, the better the classifier will be 
        
# 11:
Sequence prediction: 
- predicting the best output at each timestep doesn't have to produce the most probable seq
    - markov assumption: output is cond. only by its direct predecessor 
    - it helps to feed previous descrete ouptut as input 

- with softmax:
    - we don't get the probablity of the whole sequence 
    - softmax normalizes the probablity after each character to 1 
    
- seq2seq beam search decoding: 
    - don't consider just one output -> remember k-best solutions 
    - find the best next solution for each of them 

    - we have k best prefixes, 
    - try to enlarge each of them with k different new outputs
    - choose the k most probable ones

- conditional random fields: compute unnormalized sequences
    - probability of the whole sequence: factorizing by subqlicks 
    - exponential number of them -> solve with help of markov assumption 

    - compute unnormalized probabilities of sequences -> softmax over them in the end

    - s(X, y, 0, A) = SUM_i,N( Ayi-1,yi  + F0(yi|X) ) 
        - F0(yi|X): output of the decoder, that's the only thing we've used so far
        - Ayi-1,yi: transition probabilies from previous ones 

    - p(y|X) = softmax_z(s(X, z)) // zs are all possible sequences of labels
    - log p(y|X) = s(X, y) - logadd_z(s(X, z))

    - the logadd across all zs is the problem 
        - compute s(..) of each prefix, compute the score of next prefix via them 
        - dynamic programming 

        - at(k) = logadd_z(S, z) = f0(yt = k|X1:t) + logadd_i(at-1(i) + Ai,k)
            - probability of all sentecens with t elements ending with k

    - the transition probabilities can be learned from gold data 
    - sort-of beam search with k = number of tags with markov assumption 

- slot filling: 
    - tagging subsequences e.g. this part of sentence is name of restaurant
    - subsequences might be longer than just one word 

    - for each slot: beginning of slot, inside a slot, ...

- connectionist temporal classification 
    - seq2seq where one sequence has way higher frequency 
    - e.g. soundwaves to phonemes 

    - higher freq -> lower freq 
    - new output class: blank, classify into extended labeling 
        - blank is separator: aaa-a--bb-a -> aaba 

    - we don't know the allignement between gold data end produced extended labelings
    - one final (gold) labeling has many corresponding extended labeling 

    - compute probability of all extended labelings that correspond to certain gold labels
    - dynamic programming: extending those that end with blanks or not 

    - for decoding we must use greedy decoding or beam search 
        - can't argmax over all labelings 
        - each corresponds to a lot of extended labelings

        - beam search over final labeling not over extended labeling

Reinforcement learning: 
- S: states, A: actions, P(St+1 = s', Rt+1 = r | St = s, At = a)
    - probability that action a will lead from state s to s' with reward r 
    - d € [0, 1] is discount factor 

- Qt(a) estimated value of action a at time t:
    - sum of rewards when action a is taken / number of times a was taken 
    - ideally converges so that it's not [t]ime dependent 

- reward: sum / exponentialy decayed sum of all possible future rewards 

- action selection 
    - greedy: at(a) = argmax_a(Qt(a)), exploit, doesn't explore 
    - e-greedy: best or random with 1-e probability

- Qn+1 = 1/n SUM1_n+1 Ri = Qn + 1/n (Rn+1 - Qn)
    - Qn+1 = Qn + a(Rn+1 - Qn) : better for dynamic environment 
    - see artificial inteligence 2 notes 
    - slowly converging to stable true information using incremental updates 

- Gt: sum of all rewards from current time onward -> all rewards curr. action leads to

- policies: polici pi(a|s) computes distribution of actions to take given state s 
- vpi(s) = Epi[Gt | St = s]

- qpi(s, a) = Epi(Gt | St = s, At = a)
            = Epi(Rt+1 + d*vpi(St+1) | St = s, At = a)) 
    - expected reward being in state s and choosing action a 

- optimal policy: pi(s) = argmax_a q(s, a)

# 12:
Monte Carlo Control 

Temporal difference methods 

Q-learning

Sarsa 

Q-learning vs Monte Carlo

Deep Q networks

REINFORCE alg

- baseline 

Actor critic 

# 13: 

