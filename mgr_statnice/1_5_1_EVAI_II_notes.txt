https://drive.google.com/drive/folders/10q-akLIsi_54HAsD3vnMDM8uzVy3Zf2l :: p77+

Genetické algoritmy, genetické a evoluční programování
- evoluční algoritmy: generické označení pro výpočetní metody inspirované evolucí
    - GA: Genetické algoritmy - původně integer based, parametry evoluce externí 
    - ES: Evoluční strategie - self adapting (učí se hyperparametry), původně real-valued
    - EP: Evoluční programování - generování programů 

    -> dnes mix, těžko zařadit pod jedno či druhé 
- populační stochastiké prohledávající algoritmy, spíš metaalgoritmus/rodina algoritmů 
- základní struktura
    - vytvoř náhodnou populaci, ohodnoť
    - dokud není splněna ukončovací podmínka (fitness, čas)
        - křížení, mutace -> noví jedinci 
        - ohodnocení nových jedinců
        - selekce nové populuce z původní a nových jedinců na základě fitness

- GA: 
    - SGA (Holland)
    - binární řetězce
    - ruletová selekce, pouze rodičovská selekce (rodiče nové populace)
        - může být přeškálovaná
        - podle pořadí a ne podle hodnoty fitness 
        - ...

    - 1 bodové křížení, bitové mutace, původně inverze (odebráno)

    - alternativy: 
        - SUS selekce (ruleta, po vybrání se posuneme o 1/n, lépe garantuje poměry)
        - turnajová selekce: prohraný se nevrací zpátky do selekce

- GP: 
    - generování programů
    - původně na syntaktických stromech
        - křížení: výměna subtrees 
        - mutace: výměna nodes, musí se zachovat arita 
        - ADF (automaticky definované funkce): dvojúrovňové, místo low-level nodes používám komplexnější ADF podstromy (předdefinované, co-evolved, ...)
    - může být i lineárně (bytecode)
        - větší problém s generování nevalidních programů, tree structure funguje jako regularizace
    - problémy: 
        - bloat: třeba nějak omezit velikost stromů: část fitness, chytré operátory, ...
        - konstanty: local search, ...
    - dnešní neuroevoluce (zejména architektur) je in a way pokračování 

- EP: Evoluční programování 
    - Lawrence 
    - Snaha vytvoři umělou inteligenci, agent representovanán konečným automatem, prostředí sekvence symbolů 
    
    - populace: konečné automaty se vstupy, fitness: úspěšnost predikce výstupu 
    - různé mutace (nové stavy, změna stavu, přechodu, ...)
    - žádné křížení 

    - turnajová selekce 
    
    - vs GA:
        - teoreticky víc universální, žádné omezení na kódování jedince 
        - velká škála mutací (od malých po velké), žádné křížení ~ není křížení jen metamutace? headless chicken (95)
            - křížení funguje u problémů, kde jsou explicitní building blocky, jinak moc ne; respektive ne líp než metamutace
    - vs ES: 
        - EP turnajová selekce naproti deterministickému odstranění nejhorších, ...
 
- ES: Evoluční strategie
    - viz téma níže 

- genotyp: soubor vlastností geneticky udržovaných v populaci 
- fenotyp: konkrétní projev genotypu v jedinci závislý na konkrétním prostředí 

Teorie schémat, pravděpodobnostní modely jednoduchého genetického algoritmu
- schéma: 
    - analýza subpopulací v GA
    - 0, 1, *: libovolný symbol 

    - schéma s r *: 2^r možných jedinců 
    - jedinec délky "m" representován: 2^m schématy
    - existuje 3^m schémat délky m 
    - v pupulaci délky m o velikosti n je 2^m až n*2^m schémat (každá pozice může být konk. pevný bod / *)

    - řád schématu o(S): počet pevných pozice
    - definující délka d(S): vzdálenost mezi první a poslední pevnou pozicí 
    - fitness F(S): průměrná fitness odpovídících jedinců _v populaci_ 

    - Věta o schématech: Krátká schémata s nadprůměrnou fitness a malým řádem se v populaci během GA exponenciálně množí. 
        - P(t): populace v čase t 
        - n velikost populace, m délka jedinců 
        - C(S, t): počet jedinců schéma v populaci; odhad P(S, t + 1)

        - Selekce (ruleta): 
            - p_s(S) = F(S) / sum_u€P(t)(F(u)) -> C(S, t+1) = C(S, t) * n * p_s(S)
            - schéma má nadprůměrnou fitness (o eps) C(S, t) * n * p_s(S) = C(S, t) * F(S)/F_avg(t) -> C(S, t+1) = C(S, t) * (1+e)

        - křížení (one-point):
            - pravděpodobnost, že schéma nepřežije křížení: d(S)/m-1 // trefí se do pevných pozic -> nejspíš se změní -> už nebude odpovídat 
            - p_surv(S) >= 1 - pc*(d(S)/m-1) // nerovnost: bod křížení se může trefit dovnitř schématu a obě části stále odpovídají
        - mutace: 
            - p_surv = (1-pm)^o(S)
            - p_surv ~= +-pm * o(S) // funguje pro pm << 1 

        -> C(S, t+1) >= C(S, t) * F(S)/F_avg(t) * (1- pc * (d(S)/m-1) - pm*o(S))

    - hypotéza o stavebních blocích: GA hledá suboptimální řešení problému rekombinací krátkých, nadprůměrných, s malých řádem schémat (building blocks)
    - důskedky VoS 
        - GA pracuje s n jedinci, ale současně implicitně s 2^m až n*2^m schématy 
        - Holland tvrdí, že počet schémat, která GA v jednom kroku zpracuje je až n^3
        
        - automaticky řeší explorace vs expoitace: skrze exponenc. lepší alokaci nadějným schématům odpovídá analytickému řešení 
            - myslelo se že hraje ~3 rukého banditu, actually více her kde se soutěží o pevné pozice
            - schémata řádu k soutěží o k pozic skrze 2^k rukého bandity -> až na to, že GA nesampluje schémata rovnoměrně 

        - problém odhadu fitness schémat
            - např. F(111*) = 2, F(0*) = 1, F(x) = 0, tak F(1*)=1/2 a F(0*)=+, jenže GA odhadne F(1*) ~ 2, protože 111* v populaci převáží 
            - kolaterální konvergence -> jakmile začneme konvergovat, tak jsou všechny samply schémat biased a nejsou samplované rovnoměrně 

- pravděpodobnostní modely
    - snahy o analytické popsání GA, alespoń SGA 
    - nejjednodušší JJGA:
        - inicializuj náhodně populaci binárních řětězců x délky l 
        - dokud nenajdeš dost dobré x
            - dokud nenaplníš novou populaci
                - vyber selekcí 2 jedince, zkříž je z s pc, jendoho potomka zahoď 
                - mutuj každý bit nového řetězce s pm 
                - vlož do populace 

        - každého jedince jde brát jako binárně zapsané číslo 

    - analýza 
        - p_i(t) = kolik procent populace zabírá číslo i 
        - s_i(t) = pravděpodobnost selekce řetězce i 
        - F, F(i, i) = f(i) | else 0, f(i): fitness jedince 
        
        - s(t) = F*p(t) / sum_j=0..2^l-1{F(j, j) * p_j(t)} // předpokládáme turnajovou selekci 
        - chceme definovat matici G, která realizuje krok JJGA: p(t+1) = G@p(t)
            - G = F @ M // fitness, mutace-křížení 

        - G=F nemáme žádnou mutaci, G=F, tak: E[p(t+1)] = s(t); E[s(t+1)] ~= F * s(t)
            - problém samplingu konečných populací, čím větší, tím víc přesné
        - G=M 
            - r(i, j, k) = z i a j vytvoříme k 
            - E[pk(t+1)] = SUM_iSUM_j..2^l-1 si(t)*sj(t)*r(i,j,k) : lze analyticky spočíst 
        
        - neznáme obecné fixed points (nemění distribuci stavů) 
    
    - konečné populace: 
        - populaci lze definovat jako stav markovovského řětězce 
        - pravděpodobnost přechodu jen na předchozím stavu, ... 
        
        - jeden stav: konkrétní populace -> populací o n jedincích délky l: n+2^l-1 over 2^l-1, matice přechodů ^2, absolutně intracable 
            - n=l=8 -> 10^28 
       

Evoluční strategie, diferenciální evoluce, koevoluce, otevřená evoluce
- původně primárně pro optimalizaci reálných funkcí 
- jako první přišla s self-adaptation, součást jedince jsou evoluční parametry 

- notace
    - M: počet jedinců 
    - L: počet nových potomků
    - R: počet rodičů každého nového jedince 

    - (M+L): M jedinců do nové populace z M+L nových i starých
    - (M, L): M nových jedinců z L nových potomků (robustnější proti lok. extrémům)

    - jedinec tvaru: C(i) = [Gn(i), Sk(i)] | první část hodnota, druhá část standardní odchylka, k = 1 | n | 2n
        - k=1: společná odchylka pro všechny parametry 
        - k=n: nekorelované mutace gemetricky se mutuje po elipse rovnoběžné s osami 
        - k=2n: korelovan0 matice, geometricky mutuje podle n-rozměrného normálního zobrazení. Elipsa + natočení. ??? není třeba plná covariační matice?

- populační cyklus: 
    - n=0, randomly init P(n) of size M 
    - evaluate fitness in P(n)
    - while not good enough: 
        - repeat l times: 
            - choose R parents 
            - crossover, mutate, eva new indiv 
        - choose new population (by the type of OS), n++

- selekce enviromentální
    - nový jedinec akceptován jen když je lepší 
- mutace: 
    - genotyp nahrazen přičtením náhodného norm. rozd. čísla s patřičnou odchylkou 
    - odchylka změněná tak, aby mutace měla úspěšnost on avg. 20pct 
- křížení: 
    - uniformní (avg) / diskrétní (hodnotu jednoho vybraného rodiče)

- CMA-ES: correlation matrix adaptation ES 
    - mean vybírání je vážený průměr úspěšných jedinců z minulé generace (větší váha lepším)
    - udržuje se korelační matice pomocí iterativního EM, efektivně PCA se zachováním všech dimenzí
    - dynamický stepsize podle úspěšnosti 

- difirenciální evuluce: 
    - každý jedinec projde mutací, křížením, selekcí
    - mutace a křížení v podstatě jeden step, v mutaci se vytvoří donor, který je pak v křížení použit; není to tak, že každý krok vytváří separátní změnu

    - mutace: posun podle ostatních
        - zvolíme 3 další jedince: xa, xb, xc
        - donor: v = xa + alpha * (xb - xc) pro f(xb) > f(xc)
    - křížení: uniformní s pojistkou 
        - křížení původního jedince s donorem
        - ve výsledku zajištěn alespoň jeden prvek z donora, zbytek náhodně s určitou pravděpodobností
    - selekce: porovnání / nahrazení lepším potomkem 

- koevoluce: 
    - více druhů se vyvíjí nezávisle na sobě 
    - těžko postihnout fitness jednotlivých jedinců pokud závisí na dalším druhu 

    - např. CoDeepNeat: 
        - evoluce subnetworks & topologií/zapojení subnetworks at the same time 
        - teoreticky by 3. úroveň mohla evolvovat váhy vrstev v subnetworks
    
    - kooperace vs. predator-prey 
        - predator pray: skoro takový GAN v kontextu evoluce
    - příklady: 
        - sorting networks vs. evil number sequences
        - iterated PD hráči 
        - hráči tic-tac-toe 
    
    - sledován fitness:
        - změna jednoho hráče vede ke změně fitness landscape ostatních 
        - nejlepšího nechávám soupeřit s nejlepšími soupeři ze všech generací, odbobně jako v learningu MAS 

    - zacyklení strategií:
        - může dojít k zacyklení, podobně jako u GAN 
        - pamatovat si n-posledních, vyhodnocovat proti sobě, ... 

- otevřená evoluce: 
    - evoluce bez konce, neexistuje ukončovací podmínka 
    - fitness funkce je dynamická, může být do jisté míry i kódování jedince 

    - velké změny: nic nebude fungovat (no free lunch theorem)
    - různé typy změn: 
        - malé změny
        - velké morfologické změny: vznikají nové extrémy, ...
        - cyklické změny 
        - katastrofické změny

    - řešení: 
        - udržovat si nehomogenní populaci, nejen nejlepšího
            - niching: fitness je relativní species, species identifikované skrze podobnost, ... 
            - dynamické druhy: evoluce jen uvnitř, explicitní niching v podstatě

    - GA většinou lepší: 
        - ?nejspíš kvůli tomu, že evo. parametry (součástí evoluce v ES) nejsou konvergované pro jeden konkrétní landscape

    - bootstrap problem: 
        - na začátku všechna řešení nulová fitness -> komplexní úkol
        - začnu lehčím úkolem -> postupně měním na komplexnější a komplexnější

        - implicitně skrze GAN~like koevoluci 


Rojové optimalizační algoritmy
- optimalizační metody inspirované přírodou, např. hejny ptáků, ryb, ...
- algoritmus: 
    - init each particle 
    - while not done: 
        gBest = max(p.pBest for each p in Particles)
        foreach p in Particles: 
            p.v = p.v + c1 * rand(0, 1) * (gBest - p.Pos) + c2 * rand(0, 1) * (p.Best - p.Pos)
        p.pBest = max_p{f(p)}

    - každá částice má rychlostní vektor, aktualizuje se podle nejlepšího umístnění částice p v historii a podle nejlepší částice obecně

- podobné jako GA
    - výměna informací jen od nějlepších částic k ostatním 
    - částice mají omezenou paměť (nejlepší pozici)

- swarm inteligence (SI)
    - sebeorganizace a dělba práce 
    - PSO, ant colony optimization, ... 
    - metaphor based alghorithms: inspirované reálnou skutečností ~ kritika jestli by bez ní byly dost dobré, aby byly zajímavé 

- Ant colony optimization: 
    - inspirované chováním mravenců při hledání potravy -> hledání cesty v grafu skrze feromony 
    - p_ij(t) = t_ij(t)^a * n_ij^b / sum_n€Jk t_in(t)^a * n_in^b
        - p_ij(t): pravděpododobnost přechodu z i, do j v čase t 
        - Jk: uzly kam mravenec může aktuálně jít 
        - t_ij: množství nevypařeného fermononu 
        - n_ij: viditelnost mezi i a j 

        - umístění feromonů: Q/Lk: Q: konstanta, Lk: cena cesty, časem se vypařují t_ij(t+1) = (1-p)*t_ij(t) + sum_k=1..m delta t_k_ij(t)
            - p: parametr vypařování, suma: přidání feromonů pokud byla hrana na cestě nějakého mravence k 
        
    - feromonova robotika: 
        - s: startovací pozice
        - s': sousední uzel s minimální hodnotou u(s)
        - s(s)++ 
            - varianty: u(s)++ //exp čas pokrytí 
            - u(s) = u(s') + 1 // poly 
            - if (u(s)) < u(s') then u(s)++
            - ...
        - přesuň se na s', repeat 2...


Memetické algoritmy, hill climbing, simulované žíhání
- memetické algoritmy: obohacení EA o lokální prohledávání uvnitř cyklu 
- inspirováno meme: ideje/myšlenky/... šířící se ve společnosti 

- dva přístupy: 
    - lamarkismus: skrze lokální hledání lepší jedinec je navrácený do populace, změna genotypu skrze fenotyp
        - není korektní w.r.t to darwinismus, biologická motivace
    - baldwinismus: z lepšího jedince vezmu jen fitness pro přežití jeho nezlepšeného genotypu, hledání potenciálu

- Hill climbing 
    - nejjednodušší forma prohledávání prostoru 
    - vygeneruji náhodný bod v okolí, jdu na něj pokud je to zlepšení w.r.t to fitness funkce 
    - varianty
        - steepest ascent (gradient descent)
        - stochastic HC: vyberu směr ruletou podle toho jak jsou strmé 
        - random restart: náhodné restarty 
        - first choice (první lepší směr)
        - ...

- Simulated annealing 
    - lokální prohledávání kombinující HC a náhodnou procházku
    - alg. náhodně zvolí stav a vydá se do něj pokud 
        - je lepší než aktuální stav 
        - je horší než aktuální stav, ale jen s pravděpodobností úměrnou od toho jak je špatný a aktuální teploty
            - výšší teplota -> vyšší šance jít do špatného stavu 
            - teplota se postupně snižuje, když se zaseknu -> zvýšit 
            - e^deltaE/T //deltaE: o kolik je stav horší

- taboo search 
    - any search, pamatuje si past navštívené lokace -> jde od nich -> dobré pro exploraci 
        - pamatuje si posledních X, posledních X nejlepších, etc. 
    - scatter search: víc paralelních searchů 
    - reference set: pamatující si nejlepší různé řešení ~ v podstatě niching 

- relativně hodně v neuroevoluci: backrop
- hledání jehly v kupce sena s nápovědou, kde by cirka mohla být (okolí, které lokálně prohledám)

- otázka efektivity: není lepší výkon/čas dát evoluci?

Aplikace evolučních algoritmů (evoluce expertních systému, neuroevoluce, řešení kombinatorických úloh, vícekriteriální optimalizace)
- expertní systémy: if: then rules 
    - formální logika, by default not fuzzy (ale může být)
    - learning classifier systems: evolvované expertní systémy 

    - michiganský přístup: jedinec je pravidlo 
    - pittsburský přístup: jedinec je množina pravidel 

    - většinou klasifikační úlohy 
    - instance mají pevnou strukturu featur (nominální, diskrétní, spojité, ...)

    - pittsburský přístup: 
        - nejbližší klasickému GA 
        - jedinec je množina pravidel řešící problém, různě velká (problém variabilních jedinců)
        - problém více matchujících pravidel: voting, priorita, ... 

        - operátory jsou komplikované: na úrovni množin X na úrovni pravidel 
        - fitness: vezme se první matchující pravidlo, klasifikuje -> např. podíl správně klasifikovaných 

        - příklad: systém GABIL 
            - nominální atributy, binární klasifikace (3 třídy)
            - pravidla (komplexy) ve formátu predikát -> třída; predikát v CNF 
            - 1100|0010|1001|1 : první atribut 1. nebo 2. hodnota, druhý 3. hodnota, třetí 1. nebo 4. pak patří do třídy 1
            - snadné genetické operace 
                - jedinec: výměna pravidel, kopírování pravidel, generalizace pravidel, smazání, ... 
                - komplex: rozdělení komplexu na selektoru, generalizace, specializace, ...
                - mutace: mutace 0<>1, rozšíření, zužení, ... 

    - michiganský přístup 
        - jedinec v populaci je pravidlo, IF-THEN; 1, 0, žolík 
        - jde proti klasické evoluci, kde celá populace konverguje k jednomu řešení (nechceme)
            - šlo by řešit skrze: Iterative rule learning: iterativně spouštíme GA
                - najdeme pravidlo, uložíme pravidlo, 
                - smažeme z trénování všechny dobře klasifikované jedince
                - spustíme GA znovu a najdeme další pravidlo, ...

        - na klasifikaci se podílí celá populace 
            - evoluce není po generacích, ale jen občas a po částech populace 
            - fáze: 
                - objevování nových pravidel (GA)
                - učení odměna/penalizace jedinců podle toho, jak si vedou při klasifikaci 

            - původně měl systém podporu pro posílání zpráv (správy součástí pravidel, fronta, ...)
            - klasifikace a odměny skrze kredity za které soutěžili o řešení, těžkopádný systém 

        -> zero classifier system 
            – Pravidla, která neodpovídají dané situaci, tak nic
            – Pravidla, která odpovídají vstupu, ale mají jiný výstup: síla se zmenší násobením konstantou 0<T<1 
            – Všemp ravidlům se zmenší síla o maloučást B 
                – Tahle síla se rozdělí rovnoměrně mezi pravidla, která minule odpověděla správně, zmenšená o faktor 0<G<1 
            – Nakonec, odpověď od systému se zmenší o B a rozdělí rovnoměrně mezi pravidla, která teď odpověděla správně

        - inovace: cover operator
            - pokud žádné pravidlo neodpovídá: vytvoří se nové pravidlo pro daný vstup 
        - XCS: 
            - zakládá fitness pravidla na přesnosti nikoli výdělku: dává šanci prosadit se pravidlům která vedou k akcím s malou odměnou 

- neuroevoluce: 
    - evoluce neuronových sítí 
    - tradičně dnes: pevná architektura, váhy skrze backprop. 

    - evoluce lze použít and/or: 
        - architektura 
        - váhy 
        - různé kombinace jednoho/druhého/obojího s jinými přístupy

        - v obou případech jde v podstatě o reinforcement learning 

    - učení vah 
        - v podstatě učení floating point GA optimalizace funkce 
        - mnoho parametrů, mnoho funkčně ekvivalentních jedinců

        - jiné formy optimalizace (backprop) fungují téměř vždy rychleji/lépe 
            - možná kombinace: lokální search skrze backprop, evoluce sdílení např. různých konvolučních filtrů; těžko zabezpečit, že se nezmění význam vrstev a nerozpadne učení 
    - učení struktury
        - prostor architektur je obtížně diferencovatelný: gradientní metody moc nefungují 
        - různé možnosti kódování: 
            - přímé kódování: binární matice synapsí / podbloků 
            - gramatické kódování: skrze gramatiky 
            - celuární kódování: genetické programování -> automat vytvoří síť (přidej neuron, uber, rozděl, ...)

        - operace 
            - růst
            - prořezávání (než se nezhorší fitness)
            - dekompozice (podsítě, moduly, ...)

    - SANE 
        - evoluce částečné sítě 
        - struktura pevná, jedna skrytá vrstva, jedinci v evoluci neurony 
        - ohodnocení: vybere se pevný počet neuronů, postaví se síť, vyhodnotí se, garance výběru neuronů, fitness neuronů průměrná sítí kterých se účastnil 
    - ESP
        - podobný princip, každá lokace neuronu má vlastní populaci
        - větší tlak na diverzitu, redundanci, ... 

    - NEAT 
        - evoluce topologie společně s vahami 
        - smysluplné křížení pomocí historických značek 
        - ochrana inovací pomocí druhů 

        - operace: 
            - přidání odebrání synapse, přidání/odebrání neuronu
            - spíš růst sítí než zmenšování 

        - každý neuron má historickou značku, značka se dědí
            - křížení: pokud je uzel (dle značky) jen v jednom rodiči, automaticky je zkopírován; pokud v obou, náhodně se vybere 

        - drží se populace druhů, soutěž pouze uvnitř druhů  

    - HYPERNEAT: 
        - vyvýjíme síť, která postaví naší wanted síť, pomocná síť CPPN
            - různé nelinearity, nejen sigmoid, ... 
            - vstup: 2D souřadnice dvou neuronů; výstup váha 

        - umožňuje explicitně zachytit geometrické závislosti problému
            - dobré pro malou robotiku, ... 
            - neškáluje na velké sítě 

    - CO/DEEP/NEAT 
        - evoluce stavebních bloků (několik málo vrstev) a blueprintů pro stavby velkých bloků 
        - spojené s backprop optimalizací vah bloků, evoluce toho jaké váhy se mají sharovat, ... 
    
    - další: 
        - evoluce RNN cells 
        - evoluce transformer bloků
        ...

- Řešení kombinatorických úloh 
    - batoh: kapacita CMAX, snaha maximální cenu aby váha < CMAX 
        - kódování: jedinci délky n: bitmapa 
        - klasické operátory křížení, mutace, ... 
        - fitness: problém -> dva členy
            - max SUMi v(i)
            - min CMAX - SUM c(i)

        - problém: mnoho jedinců není validní řešení 
        - -> změna kódování na: 1 znamená dej do batohu, pokud se nepřeplní -> všechno jsou přípustná řešení 
    
    - TSP
        - fitness: délka cesty 
        - kódování: různé problémy 
            - sousednost: město j je na pozici i vede-li cesta z i do j 
                - nefunguje křížení, fungují schémata (*3* značí všechny cesty s hranou 2->3)
            - buffer: pořadí města v bufferu, postupně se mažou buffer: 123456 a repre 1121 kóduje 1-2-4-3
                - lze použít jednobodové křížení 
            - permutace 
                - nefunguje křížení, lze použít speciální operátory 
                
                - partially mapped crossover: 
                    - vybereme náhodný podřetězec z jednoho rodiče, podíváme se na tytéz pozice v druhém rodiči 
                    - pokud nejsou v potomkovi: budeme pro ně hledat umístění, ... (viz https://drive.google.com/drive/folders/10q-akLIsi_54HAsD3vnMDM8uzVy3Zf2l)
                - OX: order crossover
                    - beze změny zkopírujeme náhodný interval z rodiče1 
                    - vezmeme nepoužité hodnoty z rodiče2 a počínaje pravým koncem je dokopírujeme do potomka 

                - CX: cyclic crossover 
                    - identifikujeme cykly mezi dvěma rodiči, do nového jedince kopírujeme po těchto cyklech
                - edge recombination 
                    - pro každé město seznam hram které do/z něj vedou a jsou puožity v nějakém z rodičů 
                    - začnu náhodným městem, připojím další, vždy vybírám město s nejméně zbývajícími hranami (odeberu)
                    - jen málokdy selže ~1 pct 
                    - vylepšení: přednost dvojnásobným hranám (v obou rodičích)

                - binární maticové (křížení: průnik, and, sjednocení, ...)
        - úspěšnost hodně závisí na inicializaci: 
            - nejbližší sousedi: hladová inicializace 
            - vkládání hran: na začátku náhodná hrana, přidej k cestě T nejbližší město mimo cestu T. Najdi hranu k-j v T, že minimalizuje rozdíl mezi k-c-j a k-j. Nahraď je. 

        - mutace: 
            - inverze !
            - vkládání města do cesty 
            - posun podcesty 
            - záměna 2 měst 
            - záměna podcest 
            - Lokální zlepšení 2-opt, 3-opt, ...
            - inicializace skrz aprox. algoritmy, ... 

    - rozvrhování 
        - matice: řádky učitelé, sloupce předměty 
        - zamýchej předměty, samýchej učitele, vyměň lepší řádky jedinců 

        - nutno respektovat hard constrains v operátorech

    - plánování
        - kódování je (zas) kritické 
            - permutace: plán je permutace pořadí výrobků (nutné dekódování do plánu, těžko designovat operátory ~ lze špatně použít TSP operátory)
            - přímá reprezentace: jedinec jako plán (speciální operátory)

- vícekriteriární optimalizace: 
    - potřeba optimalizovat víc fitness funkcí najednou 
    - x slabě dominuje řešení y pokud je ve všech kritériích f_i(x) >= f_i(y)
    - x silně dominuje y: slabě dominuje + E j: f_j(x) > f_j(y)
    - řešení jsou nesrovnatelná: ani jedno demoninuje druhé 

    - pareto optimální fronta: fronta řešení, které nejsou dominována jiným jedincem  

    - řešení: 
        - agregace fitness do jednoho skaláru: obvious problémy 
        - VEGA: 
            - populaci N jedinců utřídím podle každé z n fitness funkcí 
            - pro každé i vyberu N/n nejlepších jedinců dle f_i, na ty aplikuju křížení, mutaci, ... 
            - obtížné udržení diverzity, snadná konvergence k extrémům v jednotlivých f_i 

        - NSGA
            - fitness se počítá podle dominance, diverzitu zaručuje explicitní niching 
            - populace rozdělená na fronty F1, F2, .... 
            - pro každého jedince i ve frontě Fk se spočte niching faktor: sh(i) = sum_j€Fk sh(i, j) | sh(i, j) = 1-(d(i, j)/d_share)^2 pro d(i, j) < dshare, else 0
                - vyjadřuje jak blízko je ostatním jedincům v populaci, 1: hodně blízkých sousedů, 0 samotný 
            - F1 mají dummy fitness dělenou niching faktorem, F2 dummy fitness menší než je fitness nejhoršího z F2 dělenou jejich niching factorem, ... 

        - NSGA II. 
            - místo nutnosti dělit dshare crowding distance: součet přes všechna kritéria of vzdálenost nejbližšího horšího a nejbližšího lepšího jedince (v každém kritériu)
            - namísto explicitního počítání fitness turnej: prvně podle fronty (menší lepší), druhak podle crowding distance (větší lepší)
            - explicitní elitismus: do nové generace se postupně kopírují fronty dokud se již nevejde celá, pak podle turnajového systému výše 

        - porovnání algoritmů: 
            - IGD: Inverted generational distance: 
                - https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8314455
                - průměrnou vzdálenost každého řešení na skutečné pareto opt. frontě k nejbližšímu řešení ve výsledné frontě 
                - různé možnosti jak získat refereční pointy "skutečné pareto opt. fronty"
                    - např. všechny nedominované řešení ze všech runů, uniformní sampling z téhož, ... 
            - hyperobjem: 
                - objem prostoru dominovaného danou množinou, pro 2D plocha "nad" body (omezené referenčním bodem společném pro všechny pro zabránění nekonečnosti) 